{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 01 Importação dos pacotes e bibliotecas"
      ],
      "metadata": {
        "id": "oflErDkXnS-h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPgb0E__X0cE"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade google-api-python-client\n",
        "!pip install langdetect\n",
        "!pip install unidecode\n",
        "!pip install langdetect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrgX-d8fkwdM",
        "outputId": "ea071525-8348-4fb0-85b0-ffa237763e4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Package rslp is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import unidecode\n",
        "import itertools\n",
        "import nltk\n",
        "import time\n",
        "import re\n",
        "\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from googleapiclient.discovery import build\n",
        "from six.moves import urllib\n",
        "from datetime import datetime\n",
        "from nltk.stem import *\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('rslp')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 02 Autenticação no ambiente Google (BiQuery / Drive)"
      ],
      "metadata": {
        "id": "yjgwIkSx7Ytz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# autenticação BQ\n",
        "from google.colab import auth\n",
        "\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "hde0WOOZ7kxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# autenticação drive\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsISt6RT7uAv",
        "outputId": "36923f51-a42f-43c1-8fc9-3b08e8cf0f6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 03 Consultas Youtube API"
      ],
      "metadata": {
        "id": "RRkxL5kqpXqc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 03.a Cria dicionário da lingua portuguesa"
      ],
      "metadata": {
        "id": "-8MuNoYc8LX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Cria lista para armazenar palavras do dicionário\n",
        "dicionario = []\n",
        "\n",
        "# Cria objeto para stemming das palavras\n",
        "stemmer = nltk.stem.RSLPStemmer()\n",
        "\n",
        "# Importar das URLs abaixo dicionários com palavras da língua portuguesa\n",
        "\n",
        "# 1º Dicionário\n",
        "url = \"https://www.ime.usp.br/~pf/dicios/br-utf8.txt\"\n",
        "file = urllib.request.urlopen(url)\n",
        "\n",
        "for line in file:\n",
        "  decoded_line = line.decode(\"utf-8\")\n",
        "  dicionario.append(decoded_line.rstrip(\"\\n\"))\n",
        "\n",
        "# 2º Dicionário\n",
        "url = \"http://200.17.137.109:8081/novobsi/Members/cicerog/disciplinas/introducao-a-programacao/arquivos-2016-1/algoritmos/Lista-de-Palavras.txt\"\n",
        "file = urllib.request.urlopen(url)\n",
        "\n",
        "for line in file:\n",
        "  decoded_line = line.decode(\"utf-8\")\n",
        "  dicionario.append(decoded_line.rstrip(\"\\n\"))\n",
        "\n",
        "# Transforma o texto para caixa baixa\n",
        "dicionario = [\" \".join(x.lower() for x in x.split()) for x in dicionario]\n",
        "\n",
        "# Remoção de pontuação\n",
        "dicionario = [x.replace('[^\\w\\s]','') for x in dicionario]\n",
        "dicionario = [x.replace('-','') for x in dicionario]\n",
        "\n",
        "# Remoção dos acentos\n",
        "dicionario = [unidecode.unidecode(x) for x in dicionario]\n",
        "\n",
        "# Stemm\n",
        "dicionario = [stemmer.stem(x) for x in dicionario]\n",
        "\n",
        "# Remove duplicatas\n",
        "dicionario = list(set(dicionario))"
      ],
      "metadata": {
        "id": "SIc2am3b8CxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 03.b Consulta vídeos"
      ],
      "metadata": {
        "id": "8447VJB_tJi0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oC6EKttwemZn"
      },
      "outputs": [],
      "source": [
        "\n",
        "# declaração da chave API\n",
        "api_key = 'Insira sua chave API aqui'\n",
        "\n",
        "# token que será procurado\n",
        "query = 'CryptoCars'\n",
        "\n",
        "# ticker do token que será procurado\n",
        "nomeCrypto = 'CCAR'\n",
        "\n",
        "# criando objeto para realizar consultas no Youtube\n",
        "youtube_object = build('youtube', 'v3', developerKey = api_key)\n",
        "\n",
        "# chamando o método search.list para recuperar resultados de pesquisa do youtube\n",
        "search_keyword = youtube_object.search().list(q=query, \n",
        "                                              part='id, snippet',\n",
        "                                              regionCode='BR', # consultas na região do Brasil\n",
        "                                              relevanceLanguage='pt', # idioma em português\n",
        "                                              maxResults=50).execute()\n",
        "\n",
        "# extrai os resultados\n",
        "results = search_keyword.get('items', [])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como o comentário pode vir desconfigurado, são necessários alguns ajustes antes da ingestão dos dados."
      ],
      "metadata": {
        "id": "bkMXSPvwsutK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HB8bigxPkKbq"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Importação das stopwords\n",
        "stop = stopwords.words('portuguese')\n",
        "stop.extend(['nao','boa','top','peter'])\n",
        "\n",
        "def transform_text(sentence):\n",
        "  '''\n",
        "  Input:\n",
        "      sentence: frases que serão pré-configuradas para um padrão mais legível\n",
        "  Output:\n",
        "      sentence: frases já pré-configuradas\n",
        "  '''\n",
        "  # Transforma o texto para caixa baixa \n",
        "  sentence = sentence.lower()\n",
        "\n",
        "  # Remove & por conta da falha em algumas decodificações\n",
        "  sentence = ' '.join(map(str,[x for x in sentence.split() if x[0] != '&']))\n",
        "\n",
        "  # Remoção de pontuação\n",
        "  sentence = sentence.replace('[^\\w\\s]',' ')\n",
        "  sentence = sentence.replace('\\r',' ')\n",
        "  sentence = sentence.replace('|',' ')\n",
        "\n",
        "  # Remoção dos acentos\n",
        "  sentence = unidecode.unidecode(sentence)\n",
        "\n",
        "  # Remoção de excesso de espaços em branco\n",
        "  sentence = re.sub(' +', ' ', sentence)\n",
        "\n",
        "  return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0IzfMvM5FzR"
      },
      "outputs": [],
      "source": [
        "\n",
        "def youtube_search_keyword(results):\n",
        "  '''\n",
        "  Input:\n",
        "      results: resultado da chamada API\n",
        "  Output:\n",
        "      df: conjunto de dados com informações sobre os vídeos consultados\n",
        "  '''\n",
        "  # lista vazia para armazenar:\n",
        "  # título do vídeo, id do vídeo, nome do canal, data da publicação do vídeo\n",
        "  title = []\n",
        "  videoId = []\n",
        "  channelTitle = []\n",
        "  description = []\n",
        "  publishedAt = []\n",
        "  \n",
        "  # extração de informações necessárias de cada objeto do resultado da chamada API\n",
        "  for result in results:\n",
        "    # objeto de resultado de vídeo\n",
        "    if result['id']['kind'] == 'youtube#video':\n",
        "      title.append(transform_text(result['snippet']['title']))\n",
        "      videoId.append(result['id']['videoId'])\n",
        "      channelTitle.append(transform_text(result['snippet']['channelTitle']))\n",
        "      description.append(transform_text(result['snippet']['description']))\n",
        "      publishedAt.append(datetime.fromisoformat(result['snippet']['publishedAt'][:-1]).strftime('%Y-%m-%d'))\n",
        "\n",
        "  df = pd.DataFrame(\n",
        "    {'title': title,\n",
        "     'videoId': videoId,\n",
        "     'channelTitle': channelTitle,\n",
        "     'description': description,\n",
        "     'publishedAt': publishedAt\n",
        "    })\n",
        "\n",
        "  return df\n",
        "\n",
        "# cria conjunto de dados com videos\n",
        "df_videos = youtube_search_keyword(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 03.c Consulta comentários do vídeos"
      ],
      "metadata": {
        "id": "UGbddmDc1fos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def remove_repeated_letters(sentence):\n",
        "  '''\n",
        "  Input:\n",
        "      sentence: frase com palavras que podem ter letras repetidas indevidamente\n",
        "  Output:\n",
        "      newSentence: frase configurada (sem repetição indevida de letras)\n",
        "  '''\n",
        "  newSentence = []\n",
        "\n",
        "  for word in sentence.split():\n",
        "\n",
        "    if stemmer.stem(word) not in dicionario:\n",
        "      word = ''.join(c[0] for c in itertools.groupby(word))\n",
        "      newSentence.append(word)\n",
        "    else:\n",
        "      newSentence.append(word)\n",
        "\n",
        "  newSentence = \" \".join(newSentence)\n",
        "\n",
        "  return newSentence"
      ],
      "metadata": {
        "id": "AGz8oJGQ95q6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Lista para NÃO submeter a Stemm\n",
        "notStemm = ['entr','compr','scam','scan']\n",
        "\n",
        "def transform_text_toStemmed(sentence):\n",
        "  '''\n",
        "  Input:\n",
        "      sentence: frase com texto original (pré-configurado)\n",
        "  Output:\n",
        "      textStemmed: frase preparada para PLN\n",
        "  '''\n",
        "  textStemmed = sentence\n",
        "\n",
        "  nmeTOKEN = 'crypto' + nomeCrypto\n",
        "\n",
        "  # Ajuste para preservar o nome do TOKEN do jogo\n",
        "  textStemmed = textStemmed.replace(nomeCrypto.lower(),nmeTOKEN)\n",
        "\n",
        "  # Remove hastags (alguns youtubers lançam sorteios com hashtag que comprometem a analise [sorteiobold ] )\n",
        "  textStemmed = ' '.join(map(str,[x for x in textStemmed.split() if x[0] != '#']))\n",
        "  \n",
        "  # Remove & por conta da falha em algumas decodificações\n",
        "  textStemmed = ' '.join(map(str,[x for x in textStemmed.split() if x[0] != '&']))\n",
        "\n",
        "  # Remoção de pontuação\n",
        "  textStemmed = textStemmed.replace('!',' ')\n",
        "  textStemmed = textStemmed.replace('.',' ')\n",
        "  textStemmed = textStemmed.replace(',',' ')\n",
        "  textStemmed = textStemmed.replace('=',' ')\n",
        "  textStemmed = textStemmed.replace('$',' ')\n",
        "  textStemmed = textStemmed.replace(')',' ')\n",
        "  textStemmed = textStemmed.replace('(',' ')\n",
        "  textStemmed = textStemmed.replace('#',' ')\n",
        "  textStemmed = textStemmed.replace(':',' ')\n",
        "  textStemmed = textStemmed.replace(';',' ')\n",
        "  textStemmed = textStemmed.replace('&quot',' ')\n",
        "  textStemmed = textStemmed.replace('&#39',' ')\n",
        "\n",
        "  # Remoção das stop words\n",
        "  stopwords = [unidecode.unidecode(x) for x in stop]\n",
        "\n",
        "  textStemmed = ' '.join(map(str,[x for x in textStemmed.split() if x not in stopwords]))\n",
        "\n",
        "  # Stemm\n",
        "  textStemmed = ' '.join(map(str,[stemmer.stem(x) if stemmer.stem(x) not in notStemm else x for x in textStemmed.split()]))\n",
        "\n",
        "  # Remoção de letras repetidas (uma após a outra), se nao estiver no dicionario\n",
        "  textStemmed = ' '.join(map(str,[remove_repeated_letters(x) for x in textStemmed.split()]))\n",
        "\n",
        "  # Remoção de palavras irrelevantes (dois caracteres ou menos)\n",
        "  textStemmed = ' '.join(map(str,[x for x in textStemmed.split() if len(x)>2]))\n",
        "\n",
        "  # Remoção de excesso de espaços em branco\n",
        "  textStemmed = ' '.join(map(str,[re.sub(' +', ' ', x) for x in textStemmed.split()]))\n",
        "\n",
        "  # TOKEN modificado\n",
        "  nmeTOKENadj = ''.join(sorted(set(nmeTOKEN), key=nmeTOKEN.index))\n",
        "  nmeTOKENadj = stemmer.stem(nmeTOKENadj)\n",
        "\n",
        "  # Recuperando alterações feitas no ajuste de nome do TOKEN\n",
        "  textStemmed = textStemmed.replace(nmeTOKENadj,nmeTOKEN)\n",
        "\n",
        "  return textStemmed"
      ],
      "metadata": {
        "id": "R2Ik_8ndGBy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_from_string(textStemmed,textDisplay):\n",
        "  '''\n",
        "  Input:\n",
        "      textStemmed: frase preparada para PLN\n",
        "      textDisplay: frase com texto original (pré-configurado)\n",
        "  Output:\n",
        "      checkFlg: se maior que 0, o comentario nao sera considerado por\n",
        "                nao passar nos filtros de relevancia\n",
        "  '''\n",
        "  \n",
        "  checkFlg = 0\n",
        "\n",
        "  # Remove comentários sem texto\n",
        "  if textStemmed ==  '':\n",
        "    checkFlg += 1\n",
        "\n",
        "  # Remove comentários extensos\n",
        "  if len(textStemmed) > 100:\n",
        "    checkFlg += 1\n",
        "\n",
        "  # Remove comentários de uma palavra\n",
        "  if len(textStemmed.split()) == 1:\n",
        "    checkFlg += 1\n",
        "\n",
        "  # Remove perguntas (nao sao relevantes)\n",
        "  if '?' in textDisplay:\n",
        "    checkFlg += 1\n",
        "\n",
        "  # Remove comentarios com finalidade que nao auxilia na analise de sentimento\n",
        "  # Ex: 'Otimo video','Excelente conteudo'.\n",
        "  remove = ['vide','conteud','parab','fal sobr','analis','explic']\n",
        "\n",
        "  for element in remove:\n",
        "    if element in textStemmed:\n",
        "      checkFlg += 1\n",
        "\n",
        "  return checkFlg"
      ],
      "metadata": {
        "id": "x5vmStbFMEI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IU-TqqUtkG0s"
      },
      "outputs": [],
      "source": [
        "# {execuçao de aprox. 04-05 min}\n",
        "\n",
        "def video_comments(list_title, list_video_id, list_authorDisplayName):\n",
        "  '''\n",
        "  Input:\n",
        "      list_title: lista com títulos dos vídeos a serem consultados\n",
        "      list_video_id: lista com id dos vídeos a serem consultados\n",
        "      list_authorDisplayName: lista com nome dos canais\n",
        "  Output:\n",
        "      df_comments: conjunto de dados com informações sobre os comentários dos vídeos consultados\n",
        "  '''\n",
        "  df_comments = pd.DataFrame()\n",
        "\n",
        "  for nmetitle, video_id, authorDisplayName in zip(list_title, list_video_id, list_authorDisplayName):\n",
        "\n",
        "    count_item = 0\n",
        "\n",
        "    # requisição para API do youtube para determinado id de video\n",
        "    request=youtube_object.commentThreads().list(part='snippet,replies',\n",
        "                                                 order= 'relevance',\n",
        "                                                 videoId=video_id\n",
        "                                                 )\n",
        "\n",
        "    title = []\n",
        "    channelTitle = []\n",
        "    publishedAt = []\n",
        "    textDisplay = []\n",
        "    textStemmed = []\n",
        "    likeCount = []\n",
        "    totalReplyCount = []\n",
        "\n",
        "    # iterar requisição para API\n",
        "    while request:\n",
        "\n",
        "      # como alguns vídeos possuem mais comentários que outros,\n",
        "      # para evitar uma concentração de comportamento/percepção muito grande\n",
        "      # em um mesmo vídeo, cria-se um limite de 500 comentários por video\n",
        "      if count_item > 500:\n",
        "        break\n",
        "\n",
        "      # se não houver problema na execução da requisição:\n",
        "      try:\n",
        "        \n",
        "        video_response = request.execute()\n",
        "      \n",
        "        for item in video_response['items']:\n",
        "\n",
        "          # o comentário não pode ser do dono do canal (qse sempre não é relevante)\n",
        "          if item['snippet']['topLevelComment']['snippet']['authorDisplayName'] != authorDisplayName:\n",
        "\n",
        "            # data da publicação do comentário\n",
        "            datePub = item['snippet']['topLevelComment']['snippet']['publishedAt']\n",
        "            datePub = datetime.fromisoformat(datePub[:-1]).strftime('%Y-%m-%d')\n",
        "\n",
        "            # comentário (submetido a função transform_text() )\n",
        "            comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
        "            comment = transform_text(comment)\n",
        "            comment = re.sub(re.compile('<.*?>'), '', comment)\n",
        "\n",
        "            # comentário (submetido a função transform_text_toStemmed() )\n",
        "            commentStemmed = transform_text_toStemmed(comment)\n",
        "\n",
        "            # número de curtidas para o comentário\n",
        "            likes = item['snippet']['topLevelComment']['snippet']['likeCount']\n",
        "            likes = int(likes)\n",
        "\n",
        "            # número de réplicas para o comentário\n",
        "            replycount = item['snippet']['totalReplyCount']\n",
        "            replycount = int(replycount)\n",
        "\n",
        "            # Motor de regra para validar se o comentário é relevante ou nao\n",
        "            if remove_from_string(commentStemmed,comment) == 0:\n",
        "\n",
        "              title.append(nmetitle)\n",
        "              channelTitle.append(authorDisplayName)\n",
        "              publishedAt.append(datePub)\n",
        "              textDisplay.append(comment)\n",
        "              textStemmed.append(commentStemmed)\n",
        "              likeCount.append(likes)\n",
        "              totalReplyCount.append(replycount)\n",
        "            \n",
        "              count_item += 1\n",
        "\n",
        "        # Repete a consulta (se existirem, carrega novos comentários)\n",
        "        if 'nextPageToken' in video_response:\n",
        "          request = youtube_object.commentThreads().list_next(request,\n",
        "                                                              video_response\n",
        "                                                              )\n",
        "        else:\n",
        "          break\n",
        "\n",
        "      # se a execução der erro (por exemplo: nao há comentarios para o video)\n",
        "      except:\n",
        "        # a execução é interrompida e o proximo dado processado\n",
        "        break\n",
        "\n",
        "    df = pd.DataFrame(\n",
        "      {'title': title,\n",
        "      'channelTitle': channelTitle,\n",
        "      'publishedAt': publishedAt,\n",
        "      'textDisplay': textDisplay,\n",
        "      'textStemmed': textStemmed,\n",
        "      'likeCount': likeCount,\n",
        "      'totalReplyCount': totalReplyCount\n",
        "      })\n",
        "    \n",
        "    df.sort_values(by=['likeCount'], ascending=False, inplace=True)\n",
        "\n",
        "    df_comments = df_comments.append(df)\n",
        "\n",
        "  df_comments.drop_duplicates(inplace=True)\n",
        "\n",
        "  return df_comments\n",
        "\n",
        "# lista com vídeos e nomes dos canais\n",
        "list_title = list(df_videos['title'])\n",
        "list_video_id = list(df_videos['videoId'])\n",
        "list_authorDisplayName = list(df_videos['channelTitle'])\n",
        "\n",
        "# cria conjunto de dados com comentários dos videos\n",
        "df_comments = video_comments(list_title, list_video_id, list_authorDisplayName)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# consumo ao fim do projeto de 370 queries"
      ],
      "metadata": {
        "id": "y6CgdGipufSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 04 Exportação do conjunto de dados para o GCP"
      ],
      "metadata": {
        "id": "7wZiqtW3943b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas_gbq\n",
        "\n",
        "from google.oauth2 import service_account\n",
        "\n",
        "def save_gbq(df, channel_name, nome_tabela, path_json_key):\n",
        "  '''\n",
        "  Input:\n",
        "      df: conjunto de dados a ser carregado no GCP\n",
        "      channel_name: nome do canal\n",
        "      nome_tabela: nome que será dado ao conjunto de dados carregado\n",
        "      path_json_key: chave json para a autenticaçao do acesso\n",
        "  '''\n",
        "  credentials = service_account.Credentials.from_service_account_file(path_json_key)\n",
        "  pandas_gbq.to_gbq(dataframe = df,\n",
        "                    destination_table = f'{channel_name}.{nome_tabela}',\n",
        "                    project_id = 'Insira o nome do seu projeto aqui',\n",
        "                    credentials = credentials,\n",
        "                    if_exists='replace')\n",
        "\n",
        "save_gbq(df_comments, 'youtube', 'df_comments', 'Insira o caminho de seu arquivo json aqui')"
      ],
      "metadata": {
        "id": "RxKOAYEO6y1X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "122408d2-62f6-4c15-e16f-f9730013dafd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1it [00:05,  5.15s/it]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "01.a. Extração API Youtube",
      "provenance": [],
      "collapsed_sections": [
        "oflErDkXnS-h"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}