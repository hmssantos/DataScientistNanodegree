{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02.a. Modelagem dos Dados",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 01 Instalação e Importação das Bibliotecas"
      ],
      "metadata": {
        "id": "zdyOrkIj8AmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from imblearn.over_sampling import RandomOverSampler"
      ],
      "metadata": {
        "id": "96P4ldkf_RaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 02 Importação do conjunto de dados"
      ],
      "metadata": {
        "id": "FOYyzSAW8HRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# autenticação BQ\n",
        "from google.colab import auth\n",
        "\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "CuUuhuPL1bUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# autenticação drive\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9K3g5XjdZSp",
        "outputId": "78a023a4-afc5-4a97-8745-65541487bae0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2QrAZmC9Kj4"
      },
      "outputs": [],
      "source": [
        "from google.cloud import bigquery\n",
        "\n",
        "client = bigquery.Client(project='Insira o nome do seu projeto aqui')\n",
        "\n",
        "train_data = client.query('''SELECT * FROM `nme_projeto.youtube.train_data`''').to_dataframe()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 03 Processamento dos dados e Modelo Estatístico"
      ],
      "metadata": {
        "id": "7ahR2vcq8U5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = train_data['textStemmed'].values\n",
        "\n",
        "# crio o objeto para a conversão da coleção de documentos brutos em uma matriz de recursos do TF-IDF.\n",
        "# o texto que será processado é o lapidado, ou seja, a variavel textStemmed do arquivo.\n",
        "# as variaveis do objeto TfidfVectorizer precisam ser identicas as usadas no programa '01.b. Tratamento dos Dados'\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1, 3), max_features=1000, min_df=5, max_df=0.7).fit(documents)"
      ],
      "metadata": {
        "id": "9v-A8rBG-vDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# aplico o objeto na lista de documentos e armazeno a informacao na variavel preditora X\n",
        "X = vectorizer.transform(documents)\n",
        "\n",
        "# armazeno a informacao de cluster na variavel de resposta Y\n",
        "y = train_data.cluster.values\n",
        "\n",
        "# crio o objeto para realizar sobreamostragem aleatória\n",
        "oversample = RandomOverSampler(random_state=42)\n",
        "\n",
        "# faz uma nova amostra do conjunto de dados.\n",
        "X_over, y_over = oversample.fit_resample(X, y)\n",
        "\n",
        "# divisao aleatória da amostra em subconjuntos de treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_over, y_over, test_size=0.3, random_state=42)\n",
        "\n",
        "# crio um objeto classificador de regressão logística\n",
        "# uso o algoritmo de treinamento com esquema one-vs-rest (OvR), habilitando a opção 'multi_class' como 'ovr'.\n",
        "classifier = LogisticRegression(\n",
        "                                multi_class='ovr',\n",
        "                                solver='lbfgs',\n",
        "                                class_weight='balanced', # usa os valores de y para ajustar automaticamente os \n",
        "                                                         # pesos inversamente proporcionais às frequências da classe nos dados de entrada \n",
        "                                random_state=42\n",
        "                                ).fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "qS0czybeAKck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 04 Avaliação do Modelo"
      ],
      "metadata": {
        "id": "26tPSWHc8Zxi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Com a matriz de confusão é possível tirar algumas métricas:\n",
        "*   Precision: Entre a classe positivo, quantos o modelo acertou.\n",
        "*   Recall: Entre a classe positiva com valor esperado, quantas estão certas.\n",
        "*   F1-score: Média harmônica entre Precision e Recall."
      ],
      "metadata": {
        "id": "DgJNodnLBOfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHvWZZomBD1u",
        "outputId": "57bdb035-7a48-4460-e09e-f0dbe37a0236"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                          precision    recall  f1-score   support\n",
            "\n",
            "                              jogo é lua       1.00      1.00      1.00        45\n",
            "                         mudança de vida       1.00      1.00      1.00        42\n",
            "otimismo com subida/valorização da moeda       1.00      1.00      1.00        48\n",
            "                   querem entrar no jogo       1.00      1.00      1.00        49\n",
            "                    sobre mercado de NFT       1.00      0.98      0.99        48\n",
            "                      sobre o jogo em si       1.00      1.00      1.00        43\n",
            "      sobre o momento de entrada no jogo       0.97      1.00      0.99        37\n",
            "                sobre perfomance no jogo       1.00      1.00      1.00        44\n",
            "\n",
            "                                accuracy                           1.00       356\n",
            "                               macro avg       1.00      1.00      1.00       356\n",
            "                            weighted avg       1.00      1.00      1.00       356\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aplicação do Modelo a nova base (df_comments)"
      ],
      "metadata": {
        "id": "oyTPgi948dfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery\n",
        "\n",
        "client = bigquery.Client(project='Insira o nome do seu projeto aqui')\n",
        "\n",
        "df_comments = client.query('''SELECT * FROM `nme_projeto.youtube.df_comments`''').to_dataframe()"
      ],
      "metadata": {
        "id": "hA9M24P18jQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def define_cluster(sentence):\n",
        "  '''\n",
        "  Input:\n",
        "      sentence: frase a ser classificada\n",
        "  Output:\n",
        "      cluster: cluster de maior probabilidade da frase estar relacionada\n",
        "  '''\n",
        "\n",
        "  document = [sentence]\n",
        "\n",
        "  # transformo a frase em vetores\n",
        "  document = vectorizer.transform(document)\n",
        "\n",
        "  # calculo a probabilidade da frase pertencer a cada um dos possiveis clusters\n",
        "  proba = classifier.predict_proba(document)\n",
        "\n",
        "  # identifico qual o indice da maior probabilidade\n",
        "  n = np.argmax(proba, axis=1)[0]\n",
        "\n",
        "  if proba[0][n] > 0.7:\n",
        "    # identifico qual o nome do cluster de maior probabilidade\n",
        "    cluster = classifier.classes_[n]\n",
        "  else:\n",
        "    cluster = 'desconhecido'\n",
        "\n",
        "  return cluster\n",
        "\n",
        "df_comments['cluster'] = df_comments.textStemmed.apply(lambda x: define_cluster(x))\n",
        "\n",
        "df_comments = df_comments[df_comments['cluster'] != 'desconhecido']"
      ],
      "metadata": {
        "id": "AbilA7o48qzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Base de treino possui %s registros\"%train_data.shape[0])\n",
        "\n",
        "# incluo os novos valores classificados, junto a base de treino (de modo a enriquece-la cada vez mais)\n",
        "nova_base = pd.concat([train_data,df_comments])\n",
        "\n",
        "nova_base.drop_duplicates(inplace=True)\n",
        "\n",
        "# Cria um dataframe de back-up\n",
        "last_train_data = train_data\n",
        "\n",
        "print(\"Nova base de treino (enriquecida) possui %s registros\"%nova_base.shape[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpsIsiMwGMDK",
        "outputId": "ccc9c6ba-6084-44e1-bce1-a253b52fb807"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base de treino possui 748 registros\n",
            "Nova base de treino (enriquecida) possui 748 registros\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas_gbq\n",
        "\n",
        "from google.oauth2 import service_account\n",
        "\n",
        "train_data = nova_base\n",
        "\n",
        "# Armazena no GCP a nova base treinada (com incremento de informações)\n",
        "def save_gbq(df, channel_name, nome_tabela, path_json_key):\n",
        "  '''\n",
        "  Input:\n",
        "      df: conjunto de dados a ser carregado no GCP\n",
        "      channel_name: nome do canal\n",
        "      nome_tabela: nome que será dado ao conjunto de dados carregado\n",
        "      path_json_key: chave json para a autenticaçao do acesso\n",
        "  '''\n",
        "  credentials = service_account.Credentials.from_service_account_file(path_json_key)\n",
        "  pandas_gbq.to_gbq(dataframe = df,\n",
        "                    destination_table = f'{channel_name}.{nome_tabela}',\n",
        "                    project_id = 'Insira o nome do seu projeto aqui',\n",
        "                    credentials = credentials,\n",
        "                    if_exists='replace')\n",
        "\n",
        "save_gbq(train_data, 'youtube', 'train_data', 'Insira o caminho de seu arquivo json aqui')\n",
        "\n",
        "# Armazena no GCP a última base treinada (antes do incremento de novas informações)\n",
        "def save_gbq(df, channel_name, nome_tabela, path_json_key):\n",
        "  '''\n",
        "  Input:\n",
        "      df: conjunto de dados a ser carregado no GCP\n",
        "      channel_name: nome do canal\n",
        "      nome_tabela: nome que será dado ao conjunto de dados carregado\n",
        "      path_json_key: chave json para a autenticaçao do acesso\n",
        "  '''\n",
        "  credentials = service_account.Credentials.from_service_account_file(path_json_key)\n",
        "  pandas_gbq.to_gbq(dataframe = df,\n",
        "                    destination_table = f'{channel_name}.{nome_tabela}',\n",
        "                    project_id = 'Insira o nome do seu projeto aqui',\n",
        "                    credentials = credentials,\n",
        "                    if_exists='replace')\n",
        "\n",
        "save_gbq(last_train_data, 'youtube', 'last_train_data', 'Insira o caminho de seu arquivo json aqui')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDuZpKJyJxdZ",
        "outputId": "b513cf2e-ce22-4dff-ba09-0a9d191d5572"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1it [00:04,  4.39s/it]\n",
            "1it [00:03,  3.29s/it]\n"
          ]
        }
      ]
    }
  ]
}